# Project 2

## Group 2

### Introduction
##### The main focus of this project is to get familiar with the data pipe line process. The project utilized AWS-EMR Cluster, AWS-S3 Bucket, Docker Container, PostgresSQL, and HttpServlet. In AWS-S3 bucket, there is a CSV file that contains raw data related to hotel booking. First, the program will load the CSV file from S3 bucket and perform Spark transformatons on the raw data and save it back to S3 bucket in CSV file format. The process of loading the CSV file from S3 will be handled by EMR instance. Second, the program will load all the CSV files that generated by Spark transformations then build a SQL table for each CSV file and save it in a remote database that located inside Docker container. Finally, the program will host Tomcat Server where user can inquiry for each table inside the database located inside Docker Container. 

### How To Perform Spark Transformation On Raw Data And Save Back To AWS-S3
> ./copyAndRunSparkJobJar.sh ~/.ssh/"keypair".pem spark-job-1.0-SNAPSHOT.jar

### How To Save Each CSV File That Contained Processed Data In Remote Database
> ./sendFromS3toDB.sh
### How To Host Tomcat Server For User Inquairy
> ./runServer.sh

### Example Of How To Operate The Application
#### Example 1: Copy Jar File To EMR Cluster And Perform Spark Transformation
##### Expected Input:
> ./copyAndRunSparkJobJar.sh ~/.ssh/spark-demo.pem spark-job-1.0-SNAPSHOT.jar
##### Expexted Output:
###### S3 Bucket




